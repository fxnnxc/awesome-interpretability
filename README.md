# Awesome Interpretability

We collect materials related to the interpretability which is an emerging field of understanding neural networks. Actually, we consider **mechanistic interpretability** in our mind. But, we think collecting general interpretability can grow the field in a better way. The tag (**M**) represent a post more related to the mecha interp. 


---
## Posts 


### Transformer Circuits 

* (**M**) Mechanistic Interpretability, Variables, and the Importance of Interpretable Bases [[post](https://transformer-circuits.pub/2022/mech-interp-essay/index.html)]
*  (**M**) A Mathematical Framework for Transformer Circuits [[post](https://transformer-circuits.pub/2021/framework/index.html)]
*  (**M**) Privileged Bases in the Transformer Residual Stream [[post](https://transformer-circuits.pub/2023/privileged-basis/index.html)]
*  (**M**) Attention Head Superposition [[post](https://transformer-circuits.pub/2023/may-update/index.html)]
*  (**M**) Distributed Representations: Composition & Superposition [[post](https://transformer-circuits.pub/2023/superposition-composition/index.html)]
*  (**M**) Toy Models of Superposition [[post](https://transformer-circuits.pub/2022/toy_model/index.html)]
*  (**M**) Softmax Linear Units  [[post](https://transformer-circuits.pub/2022/solu/index.html#section-3-2)]
*  (**M**) In-context Learning and Induction Heads [[post](https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html)]
*  (**M**) Superposition, Memorization, and Double Descent  [[post](https://transformer-circuits.pub/2023/toy-double-descent/index.html)]
*  (**M**) Interpretability Dreams [[post](https://transformer-circuits.pub/2023/interpretability-dreams/index.html)]

### CNN Circuits 

*  (**M**) Zoom In: An Introduction to Circuits [[post](https://distill.pub/2020/circuits/zoom-in/#claim-1-polysemantic)]
*  (**M**) Visualizing Weights [[post](https://distill.pub/2020/circuits/visualizing-weights/)]
*  (**M**) Weight Banding [[post](https://distill.pub/2020/circuits/weight-banding/)]
*  (**M**) Curve Circuits [[post](https://distill.pub/2020/circuits/curve-circuits/)]
*  (**M**) Branch Specialization [[post](https://distill.pub/2020/circuits/branch-specialization/)]



### Distill

* (**M**) Exploring Neural Networks with Activation Atlases [[post](https://distill.pub/2019/activation-atlas/)]
* (**M**) Attention and Augmented Recurrent Neural Networks, [[post](https://distill.pub/2016/augmented-rnns/)]
* A Discussion of Adversarial Examples Are Not Bugs, They Are Features [[post](https://distill.pub/2019/advex-bugs-discussion/)]
* Using Artificial Intelligence to Augment Human Intelligence [[post](https://distill.pub/2017/aia/)]
* Adversarial : Discussion and Author Responses [[post](https://distill.pub/2019/advex-bugs-discussion/original-authors/#citation)]


### LessWrong & AI Alignment Forum

*  (**M**)  Inside the mind of a superhuman Go model: How does Leela Zero read ladders? [[post](https://www.lesswrong.com/posts/FF8i6SLfKb4g7C4EL/inside-the-mind-of-a-superhuman-go-model-how-does-leela-zero-2)]
* (**M**)  A Mechanistic Interpretability Analysis of a GridWorld Agent-Simulator [[post](https://www.lesswrong.com/posts/JvQWbrbPjuvw4eqxv/a-mechanistic-interpretability-analysis-of-a-gridworld-agent)]
*   (**M**) Singular Value Decompositions of Transformer Weight Matrices [[post](https://www.lesswrong.com/posts/mkbGjzxD8d8XqKHzA/the-singular-value-decompositions-of-transformer-weight)]
*  (**M**)  interpreting GPT: the logit lens [[post](https://www.lesswrong.com/posts/AcKRB8wDpdaN6v6ru/interpreting-gpt-the-logit-lens)]
*  (**M**) We Found An Neuron in GPT-2 [[post](https://www.lesswrong.com/posts/cgqh99SHsCv3jJYDS/we-found-an-neuron-in-gpt-2)]
*  (**M**)  Taking features out of superposition with sparse autoencoders [[post](https://www.alignmentforum.org/posts/z6QQJbtpkEAX3Aojj/interim-research-report-taking-features-out-of-superposition)]
*  (**M**)  Attribution Patching: Activation Patching At Industrial Scale [[post](https://www.lesswrong.com/posts/gtLLBhzQTG6nKTeCZ/attribution-patching-activation-patching-at-industrial-scale)]
*  (**M**) How to Think About Activation Patching [[post](https://www.lesswrong.com/posts/xh85KbTFhbCz7taD4/how-to-think-about-activation-patching)]
*  (**M**) Current themes in mechanistic interpretability research [[post](https://www.alignmentforum.org/posts/Jgs7LQwmvErxR9BCC/current-themes-in-mechanistic-interpretability-research)]
*  (**M**) A Mechanistic Interpretability Analysis of Grokking [[post](https://www.alignmentforum.org/posts/N6WM6hs7RQMKDhYjB/a-mechanistic-interpretability-analysis-of-grokking)]
*  (**M**) Decision Transformer Interpretability [[post](https://www.lesswrong.com/posts/bBuBDJBYHt39Q5zZy/decision-transformer-interpretability)]
* (**M**) Re-Examining LayerNorm [[post](https://www.alignmentforum.org/posts/jfG6vdJZCwTQmG7kb/re-examining-layernorm)]
*  (**M**) An Analogy for Understanding Transformers [[post](https://www.lesswrong.com/posts/euam65XjigaCJQkcN/an-analogy-for-understanding-transformers)]
*  (**M**) Fundamental' vs 'applied' mechanistic interpretability research [[post](https://www.lesswrong.com/posts/uvEyizLAGykH8LwMx/fundamental-vs-applied-mechanistic-interpretability-research)]
* What Is The True Name of Modularity? [[post](https://www.lesswrong.com/posts/TTTHwLpcewGjQHWzh/what-is-the-true-name-of-modularity)]
* Deep learning models might be secretly (almost) linear [[post](https://www.lesswrong.com/posts/JK9nxcBhQfzEgjjqe/deep-learning-models-might-be-secretly-almost-linear)]
* Dropout can create a privileged basis in the ReLU output model [[post](https://www.lesswrong.com/posts/uSdFFTATPFJz4pQyB/dropout-can-create-a-privileged-basis-in-the-relu-output)]
* Residual stream norms grow exponentially over the forward pass [[post](https://www.lesswrong.com/posts/8mizBCm3dyc432nK8/residual-stream-norms-grow-exponentially-over-the-forward)]
* Basic facts about language models during training [[post](https://www.lesswrong.com/posts/2JJtxitp6nqu6ffak/basic-facts-about-language-models-during-training-1)]
* GPTs are Predictors, not Imitators [[post](https://www.alignmentforum.org/posts/nH4c3Q9t9F3nJ7y8W/gpts-are-predictors-not-imitators)]
* LLM Modularity: The Separability of Capabilities in Large Language Models [[post](https://www.lesswrong.com/posts/j84JhErNezMxyK4dH/llm-modularity-the-separability-of-capabilities-in-large)]

### Other Blogs 

*  (**M**)  Language models can explain neurons in language models, OpenAI  [[post](https://openaipublic.blob.core.windows.net/neuron-explainer/paper/index.html)]
* Neural Networks, Manifolds, and Topology, Colah's Blog  [[post](https://colah.github.io/posts/2014-03-NN-Manifolds-Topology/)]
* Decoding The Thought Vector, Gabgoh  [[post](https://gabgoh.github.io/ThoughtVectors/)]
* Johnsonâ€“Lindenstrauss lemma, Wiki  [[post](https://en.wikipedia.org/wiki/Johnson%E2%80%93Lindenstrauss_lemma)]
* Compressed sensing, Wiki [[post](https://en.wikipedia.org/wiki/Compressed_sensing)]
* An Extremely Opinionated Annotated List of My Favourite Mechanistic Interpretability Papers, Neel Nanda [[post](https://www.neelnanda.io/mechanistic-interpretability/favourite-papers)]
* Adversarial Examples Are Not Bugs, They Are Features, Gradientscience [[post](http://gradientscience.org/adv/)]


---
## Papers 


