# Awesome Interpretability

---
## Posts 


### Transformer Circuits 

* Mechanistic Interpretability, Variables, and the Importance of Interpretable Bases [[post](https://transformer-circuits.pub/2022/mech-interp-essay/index.html)]
* A Mathematical Framework for Transformer Circuits [[post](https://transformer-circuits.pub/2021/framework/index.html)]
* Privileged Bases in the Transformer Residual Stream [[post](https://transformer-circuits.pub/2023/privileged-basis/index.html)]
* Attention Head Superposition [[post](https://transformer-circuits.pub/2023/may-update/index.html)]
* Distributed Representations: Composition & Superposition [[post](https://transformer-circuits.pub/2023/superposition-composition/index.html)]
* Toy Models of Superposition [[post](https://transformer-circuits.pub/2022/toy_model/index.html)]
* Softmax Linear Units  [[post](https://transformer-circuits.pub/2022/solu/index.html#section-3-2)]
* In-context Learning and Induction Heads [[post](https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html)]
* Superposition, Memorization, and Double Descent  [[post](https://transformer-circuits.pub/2023/toy-double-descent/index.html)]
* Interpretability Dreams [[post](https://transformer-circuits.pub/2023/interpretability-dreams/index.html)]

### CNN Circuits 

* Zoom In: An Introduction to Circuits [[post](https://distill.pub/2020/circuits/zoom-in/#claim-1-polysemantic)]
* Visualizing Weights [[post](https://distill.pub/2020/circuits/visualizing-weights/)]
* Weight Banding [[post](https://distill.pub/2020/circuits/weight-banding/)]
* Curve Circuits [[post](https://distill.pub/2020/circuits/curve-circuits/)]
* Branch Specialization [[post](https://distill.pub/2020/circuits/branch-specialization/)]



### Distill

* Exploring Neural Networks with Activation Atlases [[post](https://distill.pub/2019/activation-atlas/)]
* A Discussion of Adversarial Examples Are Not Bugs, They Are Features [[post](https://distill.pub/2019/advex-bugs-discussion/)]
* Using Artificial Intelligence to Augment Human Intelligence [[post](https://distill.pub/2017/aia/)]
* Adversarial : Discussion and Author Responses [[post](https://distill.pub/2019/advex-bugs-discussion/original-authors/#citation)]
* Attention and Augmented Recurrent Neural Networks, [[post](https://distill.pub/2016/augmented-rnns/)]


### LessWrong & AI Alignment Forum

* Deep learning models might be secretly (almost) linear [[post](https://www.lesswrong.com/posts/JK9nxcBhQfzEgjjqe/deep-learning-models-might-be-secretly-almost-linear)]
* Re-Examining LayerNorm [[post](https://www.alignmentforum.org/posts/jfG6vdJZCwTQmG7kb/re-examining-layernorm)]
* Dropout can create a privileged basis in the ReLU output model [[post](https://www.lesswrong.com/posts/uSdFFTATPFJz4pQyB/dropout-can-create-a-privileged-basis-in-the-relu-output)]
* Residual stream norms grow exponentially over the forward pass [[post](https://www.lesswrong.com/posts/8mizBCm3dyc432nK8/residual-stream-norms-grow-exponentially-over-the-forward)]
* Fundamental' vs 'applied' mechanistic interpretability research [[post](https://www.lesswrong.com/posts/uvEyizLAGykH8LwMx/fundamental-vs-applied-mechanistic-interpretability-research)]
* An Analogy for Understanding Transformers [[post](https://www.lesswrong.com/posts/euam65XjigaCJQkcN/an-analogy-for-understanding-transformers)]
* Basic facts about language models during training [[post](https://www.lesswrong.com/posts/2JJtxitp6nqu6ffak/basic-facts-about-language-models-during-training-1)]
* GPTs are Predictors, not Imitators [[post](https://www.alignmentforum.org/posts/nH4c3Q9t9F3nJ7y8W/gpts-are-predictors-not-imitators)]
* LLM Modularity: The Separability of Capabilities in Large Language Models [[post](https://www.lesswrong.com/posts/j84JhErNezMxyK4dH/llm-modularity-the-separability-of-capabilities-in-large)]
* Inside the mind of a superhuman Go model: How does Leela Zero read ladders? [[post](https://www.lesswrong.com/posts/FF8i6SLfKb4g7C4EL/inside-the-mind-of-a-superhuman-go-model-how-does-leela-zero-2)]
* A Mechanistic Interpretability Analysis of a GridWorld Agent-Simulator [[post](https://www.lesswrong.com/posts/JvQWbrbPjuvw4eqxv/a-mechanistic-interpretability-analysis-of-a-gridworld-agent)]
* Singular Value Decompositions of Transformer Weight Matrices [[post](https://www.lesswrong.com/posts/mkbGjzxD8d8XqKHzA/the-singular-value-decompositions-of-transformer-weight)]
* interpreting GPT: the logit lens [[post](https://www.lesswrong.com/posts/AcKRB8wDpdaN6v6ru/interpreting-gpt-the-logit-lens)]
* We Found An Neuron in GPT-2 [[post](https://www.lesswrong.com/posts/cgqh99SHsCv3jJYDS/we-found-an-neuron-in-gpt-2)]
* Taking features out of superposition with sparse autoencoders [[post](https://www.alignmentforum.org/posts/z6QQJbtpkEAX3Aojj/interim-research-report-taking-features-out-of-superposition)]
* Attribution Patching: Activation Patching At Industrial Scale [[post](https://www.lesswrong.com/posts/gtLLBhzQTG6nKTeCZ/attribution-patching-activation-patching-at-industrial-scale)]
* How to Think About Activation Patching [[post](https://www.lesswrong.com/posts/xh85KbTFhbCz7taD4/how-to-think-about-activation-patching)]
* Current themes in mechanistic interpretability research [[post](https://www.alignmentforum.org/posts/Jgs7LQwmvErxR9BCC/current-themes-in-mechanistic-interpretability-research)]
* A Mechanistic Interpretability Analysis of Grokking [[post](https://www.alignmentforum.org/posts/N6WM6hs7RQMKDhYjB/a-mechanistic-interpretability-analysis-of-grokking)]
* Decision Transformer Interpretability [[post](https://www.lesswrong.com/posts/bBuBDJBYHt39Q5zZy/decision-transformer-interpretability)]
* What Is The True Name of Modularity? [[post](https://www.lesswrong.com/posts/TTTHwLpcewGjQHWzh/what-is-the-true-name-of-modularity)]


### Other Blogs 

* Language models can explain neurons in language models, OpenAI  [[post](https://openaipublic.blob.core.windows.net/neuron-explainer/paper/index.html)]
* Neural Networks, Manifolds, and Topology, Colah's Blog  [[post](https://colah.github.io/posts/2014-03-NN-Manifolds-Topology/)]
* Decoding The Thought Vector, Gabgoh  [[post](https://gabgoh.github.io/ThoughtVectors/)]
* Johnsonâ€“Lindenstrauss lemma, Wiki  [[post](https://en.wikipedia.org/wiki/Johnson%E2%80%93Lindenstrauss_lemma)]
* Compressed sensing, Wiki [[post](https://en.wikipedia.org/wiki/Compressed_sensing)]
* An Extremely Opinionated Annotated List of My Favourite Mechanistic Interpretability Papers, Neel Nanda [[post](https://www.neelnanda.io/mechanistic-interpretability/favourite-papers)]
* Adversarial Examples Are Not Bugs, They Are Features, Gradientscience [[post](http://gradientscience.org/adv/)]
* 


---
## Papers 


